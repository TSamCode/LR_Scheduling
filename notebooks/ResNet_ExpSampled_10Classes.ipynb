{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook_PrecisionMetric_Dog_Truck.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1856025e1e484c718b3f94fd802abb3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c839ae3a3269494786a97f35ba6bb21a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18880c59a86740e4a9b8cf8ed6ef4b32",
              "IPY_MODEL_7afcf9b3a85743588312c652603a9a43",
              "IPY_MODEL_25e57026654d42449d8e2706c43dbe99"
            ]
          }
        },
        "c839ae3a3269494786a97f35ba6bb21a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18880c59a86740e4a9b8cf8ed6ef4b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_722c9ff306ba4999bc11b0db125e077e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3aa1855dc834b2daba96362364ce41d"
          }
        },
        "7afcf9b3a85743588312c652603a9a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_843669b09716457db023687dad78107d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85e1a0f0d5bb4974b0a014b50bbebcd3"
          }
        },
        "25e57026654d42449d8e2706c43dbe99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_145e66d51f1d4d5eaf7ada2e557e5cb6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:04&lt;00:00, 47861386.58it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a88d1c2aa19f4d3694bc21f81c115f7d"
          }
        },
        "722c9ff306ba4999bc11b0db125e077e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3aa1855dc834b2daba96362364ce41d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "843669b09716457db023687dad78107d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85e1a0f0d5bb4974b0a014b50bbebcd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "145e66d51f1d4d5eaf7ada2e557e5cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a88d1c2aa19f4d3694bc21f81c115f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5vTEiRMDqDh"
      },
      "source": [
        "**Import CIFAR-100 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hr__YRUdTBh"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "class IMBALANCECIFAR10(torchvision.datasets.CIFAR10):\n",
        "    cls_num = 10\n",
        "\n",
        "    def __init__(self, root, imb_type='exp', imb_factor=0.01, rand_number=0, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super(IMBALANCECIFAR10, self).__init__(root, train, transform, target_transform, download)\n",
        "        np.random.seed(rand_number)\n",
        "        img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imb_factor)\n",
        "        self.gen_imbalanced_data(img_num_list)\n",
        "\n",
        "    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n",
        "        img_max = len(self.data) / cls_num\n",
        "        img_num_per_cls = []\n",
        "        if imb_type == 'exp':\n",
        "            for cls_idx in range(cls_num):\n",
        "                num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
        "                img_num_per_cls.append(int(num))\n",
        "        elif imb_type == 'step':\n",
        "            for cls_idx in range(cls_num // 2):\n",
        "                img_num_per_cls.append(int(img_max))\n",
        "            for cls_idx in range(cls_num // 2):\n",
        "                img_num_per_cls.append(int(img_max * imb_factor))\n",
        "        else:\n",
        "            img_num_per_cls.extend([int(img_max)] * cls_num)\n",
        "        return img_num_per_cls\n",
        "\n",
        "    def gen_imbalanced_data(self, img_num_per_cls):\n",
        "        new_data = []\n",
        "        new_targets = []\n",
        "        targets_np = np.array(self.targets, dtype=np.int64)\n",
        "        classes = np.unique(targets_np)\n",
        "        # np.random.shuffle(classes)\n",
        "        self.num_per_cls_dict = dict()\n",
        "        for the_class, the_img_num in zip(classes, img_num_per_cls):\n",
        "            self.num_per_cls_dict[the_class] = the_img_num\n",
        "            idx = np.where(targets_np == the_class)[0]\n",
        "            np.random.shuffle(idx)\n",
        "            selec_idx = idx[:the_img_num]\n",
        "            new_data.append(self.data[selec_idx, ...])\n",
        "            new_targets.extend([the_class, ] * the_img_num)\n",
        "        new_data = np.vstack(new_data)\n",
        "        self.data = new_data\n",
        "        self.targets = new_targets\n",
        "        \n",
        "    def get_cls_num_list(self):\n",
        "        cls_num_list = []\n",
        "        for i in range(self.cls_num):\n",
        "            cls_num_list.append(self.num_per_cls_dict[i])\n",
        "        return cls_num_list"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he80k8vSdlIc"
      },
      "source": [
        "def create_sampled_CIFAR10_data():\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "\n",
        "    transform_val = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "\n",
        "    trainset = IMBALANCECIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    testset = IMBALANCECIFAR10(root='./data', train=False, download=True, transform=transform_val)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)    \n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainset, trainloader, testset, testloader"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "1856025e1e484c718b3f94fd802abb3e",
            "c839ae3a3269494786a97f35ba6bb21a",
            "18880c59a86740e4a9b8cf8ed6ef4b32",
            "7afcf9b3a85743588312c652603a9a43",
            "25e57026654d42449d8e2706c43dbe99",
            "722c9ff306ba4999bc11b0db125e077e",
            "d3aa1855dc834b2daba96362364ce41d",
            "843669b09716457db023687dad78107d",
            "85e1a0f0d5bb4974b0a014b50bbebcd3",
            "145e66d51f1d4d5eaf7ada2e557e5cb6",
            "a88d1c2aa19f4d3694bc21f81c115f7d"
          ]
        },
        "id": "mzbW0jPFgmlj",
        "outputId": "3a7db310-4b10-43f3-e3a7-85d175717004"
      },
      "source": [
        "trainset, trainloader, testset, testloader = create_sampled_CIFAR10()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1856025e1e484c718b3f94fd802abb3e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9qkknck0Oh"
      },
      "source": [
        "def create_class_subsets(trainset, shuffle=True, batch_size=128):\n",
        "    \n",
        "    labels = np.array(trainset.targets)\n",
        "    \n",
        "    trainset_0 = torch.utils.data.Subset(trainset, list(np.where(labels == 0)[0]))\n",
        "    trainset_1 = torch.utils.data.Subset(trainset, list(np.where(labels == 1)[0]))\n",
        "    trainset_2 = torch.utils.data.Subset(trainset, list(np.where(labels == 2)[0]))\n",
        "    trainset_3 = torch.utils.data.Subset(trainset, list(np.where(labels == 3)[0]))\n",
        "    trainset_4 = torch.utils.data.Subset(trainset, list(np.where(labels == 4)[0]))\n",
        "    trainset_5 = torch.utils.data.Subset(trainset, list(np.where(labels == 5)[0]))\n",
        "    trainset_6 = torch.utils.data.Subset(trainset, list(np.where(labels == 6)[0]))\n",
        "    trainset_7 = torch.utils.data.Subset(trainset, list(np.where(labels == 7)[0]))\n",
        "    trainset_8 = torch.utils.data.Subset(trainset, list(np.where(labels == 8)[0]))\n",
        "    trainset_9 = torch.utils.data.Subset(trainset, list(np.where(labels == 9)[0]))\n",
        "\n",
        "    trainsets = [trainset_0, trainset_1, trainset_2, trainset_3, trainset_4, trainset_5, trainset_6, trainset_7, trainset_8, trainset_9]\n",
        "\n",
        "    trainloader_0 = torch.utils.data.DataLoader(trainset_0, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    trainloader_1 = torch.utils.data.DataLoader(trainset_1, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    trainloader_2 = torch.utils.data.DataLoader(trainset_2, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    trainloader_3 = torch.utils.data.DataLoader(trainset_3, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    trainloader_4 = torch.utils.data.DataLoader(trainset_4, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    trainloader_5 = torch.utils.data.DataLoader(trainset_5, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    trainloader_6 = torch.utils.data.DataLoader(trainset_6, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    trainloader_7 = torch.utils.data.DataLoader(trainset_7, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    trainloader_8 = torch.utils.data.DataLoader(trainset_8, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    trainloader_9 = torch.utils.data.DataLoader(trainset_9, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "\n",
        "    trainloaders = [trainloader_0, trainloader_1, trainloader_2, trainloader_3, trainloader_4, trainloader_5, trainloader_6, trainloader_7, trainloader_8, trainloader_9]\n",
        "\n",
        "    return trainsets, trainloaders\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeXLlxJzlibh",
        "outputId": "02e28f93-8ea1-4171-df49-63dae3a18a71"
      },
      "source": [
        "trainset_0, trainset_1, trainset_2, trainset_3, trainset_4, trainset_5, trainset_6, trainset_7, trainset_8, trainset_9, trainloader_0, trainloader_1, trainloader_2, trainloader_3, trainloader_4, trainloader_5, trainloader_6, trainloader_7, trainloader_8, trainloader_9 = create_class_subsets(trainset, shuffle=True, batch_size=128)\n",
        "testset_0, testset_1, testset_2, testset_3, testset_4, testset_5, testset_6, testset_7, testset_8, testset_9, testloader_0, testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6, testloader_7, testloader_8, testloader_9 = create_class_subsets(testset, shuffle=False, batch_size=100)\n",
        "\n",
        "print(len(testset_1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOPrjCpKEJ6L"
      },
      "source": [
        "**Creating ResNet model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsa_2s2JELwl"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from typing import Type, Any, Callable, Union, List, Optional"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJz8WbjREP2n"
      },
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    '''\n",
        "    Implementation is taken from the PyTorch GitHub repository\n",
        "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "    '''\n",
        "\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    '''\n",
        "    Implementation is taken from the PyTorch GitHub repository\n",
        "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "    '''    \n",
        "    \n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_C4EOlXEVua"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    '''\n",
        "    Implementation is taken from the PyTorch GitHub repository\n",
        "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "    '''\n",
        "    \n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None) -> None:\n",
        "        super(BasicBlock, self).__init__()\n",
        "        \n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        \n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        \n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        \n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRJVEro9EaRL"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    '''\n",
        "    Implementation is taken from the PyTorch GitHub repository\n",
        "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "    '''\n",
        "\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None) -> None:\n",
        "        super(Bottleneck, self).__init__()\n",
        "        \n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        \n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        \n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c2das5zEl8c"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    '''\n",
        "    Implementation is taken from the PyTorch GitHub repository\n",
        "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "    '''\n",
        "\n",
        "\n",
        "    def __init__(self, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], num_classes: int = 1000, zero_init_residual: bool = False, groups: int = 1, width_per_group: int = 64, replace_stride_with_dilation: Optional[List[bool]] = None, norm_layer: Optional[Callable[..., nn.Module]] = None) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        \n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        \n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        \n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        \n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        \n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
        "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
        "        \n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        \n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        \n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        \n",
        "        self.inplanes = planes * block.expansion\n",
        "        \n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrBpCBspEodE"
      },
      "source": [
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2], 10)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDS-eGkNsskk"
      },
      "source": [
        "def train_congestion_avoider_10classes(trainloaders, device, model, optimizer, criterion, boolean_values, grads, epoch_counts):\n",
        "\n",
        "    ''' \n",
        "        XXXX\n",
        "    '''\n",
        "\n",
        "    import copy\n",
        "\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    cls_num = len(trainloaders)\n",
        "    confusion_matrix = np.zeros((cls_num, cls_num))\n",
        "\n",
        "    for epoch_count, boolean in zip(epoch_counts, boolean_values):\n",
        "        if boolean:\n",
        "            epoch_count = 0\n",
        "    \n",
        "    for cls_num, trainloader in enumerate(trainloaders):\n",
        "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs,targets)\n",
        "        \n",
        "            # Back-propagate the loss due to 'cats'\n",
        "            loss.backward(retain_graph=True)\n",
        "            with torch.no_grad():\n",
        "                for name, parameter in model.named_parameters():\n",
        "                  try:\n",
        "                      if name not in grads[cls_num].keys():\n",
        "                          grads[cls_num][name] = torch.mul(copy.deepcopy(parameter.grad), optimizer.param_groups[0]['lr'])\n",
        "                      else:\n",
        "                          grads[cls_num][name] += torch.mul(copy.deepcopy(parameter.grad), optimizer.param_groups[0]['lr'])\n",
        "                  except:\n",
        "                      pass\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            for target, pred in zip(targets, predicted):\n",
        "                confusion_matrix[target][pred] += 1\n",
        "\n",
        "    accuracies = np.zeros((10))\n",
        "    recalls = np.zeros((10))\n",
        "    precisions = np.zeros((10))\n",
        "    fScores = np.zeros((10))\n",
        "\n",
        "    for epoch_count in epoch_counts:\n",
        "      epoch_count += 1\n",
        "\n",
        "    for cls in range(cls_num):\n",
        "      accuracies[cls] = confusion_matrix[cls][cls] / confusion_matrix.sum()\n",
        "      recalls[cls] = confusion_matrix[cls][cls] / confusion_matrix.sum(0)[cls]\n",
        "      precisions[cls] = confusion_matrix[cls][cls] / confusion_matrix.sum(1)[cls]\n",
        "      try:\n",
        "        fScores[cls] = 2 * precisions[cls] * recalls[cls] / (precisions[cls] + recalls[cls])\n",
        "      except:\n",
        "        fScores[cls] = 0\n",
        "\n",
        "    return confusion_matrix, accuracies, recalls, precisions, fScores, grads, epoch_counts"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4o2AvVubk5R"
      },
      "source": [
        "def linear_cong_condition(min_cond, max_cond, epoch, max_epochs):\n",
        "\n",
        "    condition = min_cond + (max_cond - min_cond) * (epoch / max_epochs)\n",
        "\n",
        "    return condition"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsWXF0dSQDup"
      },
      "source": [
        "def congestion_avoid(model, optimizer, metrics, condition, grads, min_epochs, mult, epoch_counts):\n",
        "    '''\n",
        "    global epoch_count_one\n",
        "    global epoch_count_two\n",
        "\n",
        "    boolean_one = False\n",
        "    boolean_two = False\n",
        "\n",
        "    branch1_cond = (branch1_metric < condition * branch2_metric) and (epoch_count_two >= min_epochs)\n",
        "    branch2_cond = (branch2_metric < condition * branch1_metric) and (epoch_count_one >= min_epochs)\n",
        "\n",
        "    if branch1_cond:\n",
        "        boolean_one = True\n",
        "        print('Branch 1 condition has been met ..... {:.2f}%'.format(100.*condition))\n",
        "        for name, value in model.named_parameters():\n",
        "            with torch.no_grad():\n",
        "                if name in branch_two_grads.keys():\n",
        "                    value += mult * branch_two_grads[name]\n",
        "        for name in branch_two_grads.keys():\n",
        "            branch_two_grads[name] -= mult * branch_two_grads[name]\n",
        "        epoch_count_two = 0\n",
        "\n",
        "    elif branch2_cond:\n",
        "        boolean_two = True\n",
        "        print('Branch 2 condition has been met ..... {:.2f}%'.format(100.*condition))\n",
        "        for name, value in model.named_parameters():\n",
        "            with torch.no_grad():\n",
        "                if name in branch_one_grads.keys():\n",
        "                    value += mult * branch_one_grads[name]\n",
        "        for name in branch_one_grads.keys():\n",
        "            branch_one_grads[name] -= mult * branch_one_grads[name]\n",
        "        epoch_count_one = 0\n",
        "    \n",
        "    else:\n",
        "        print('No condition is met ..... {:.2f}%'.format(100.*condition))'''\n",
        "\n",
        "    return"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34b4yc4-PeDL"
      },
      "source": [
        "def test_congestion_avoider_10classes(start_time, testloaders, device, model, optimizer, scheduler, grads, criterion, epoch, max_epochs, min_cond, max_cond, min_epochs, mult, epoch_counts):\n",
        "\n",
        "    ''' \n",
        "        XXXX\n",
        "    '''\n",
        "\n",
        "    import copy\n",
        "\n",
        "    model.eval()\n",
        "    cls_num = len(testloaders)\n",
        "    confusion_matrix = np.zeros((cls_num, cls_num))\n",
        "    accuracies = np.zeros((10))\n",
        "    recalls = np.zeros((10))\n",
        "    precisions = np.zeros((10))\n",
        "    fScores = np.zeros((10))\n",
        "    boolean_values = np.zeros((10))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for cls_num, testloader in enumerate(testloaders):\n",
        "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "            \n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs,targets)\n",
        "                _, predicted = outputs.max(1)\n",
        "\n",
        "                for target, pred in zip(targets, predicted):\n",
        "                    confusion_matrix[target][pred] += 1\n",
        "    \n",
        "        for cls in range(cls_num):\n",
        "            accuracies[cls] = confusion_matrix[cls][cls] / confusion_matrix.sum() \n",
        "            recalls[cls] = confusion_matrix[cls][cls] / confusion_matrix.sum(0)[cls]\n",
        "            precisions[cls] = confusion_matrix[cls][cls] / confusion_matrix.sum(1)[cls]\n",
        "            try:\n",
        "                fScores[cls] = 2 * precisions[cls] * recalls[cls] / (precisions[cls] + recalls[cls])\n",
        "            except:\n",
        "                fScores[cls] = 0\n",
        "\n",
        "        condition = linear_cong_condition(min_cond, max_cond, epoch, max_epochs)\n",
        "        congestion_avoid(model, optimizer, precisions, condition, grads, min_epochs, mult, epoch_counts)\n",
        "        scheduler.step()\n",
        "\n",
        "        print('time: %.3f sec'% ((time.time()-start_time)))\n",
        "        print(confusion_matrix)\n",
        "        for cls in range(cls_num):\n",
        "            print('Class %d A: : %.3f%% (%d/%d)'%(cls, 100*accuracies[cls], confusion_matrix[cls][cls], confusion_matrix.sum()))\n",
        "            print('Class %d P: : %.3f%% (%d/%d)'%(cls, 100*precisions[cls], confusion_matrix[cls][cls], confusion_matrix.sum(1)[cls]))\n",
        "            print('Class %d R: : %.3f%% (%d/%d)'%(cls, 100*recalls[cls], confusion_matrix[cls][cls], confusion_matrix.sum(0)[cls]))\n",
        "            print('Class %d F: : %.3f%%'%(cls, 100*fScores[cls]))\n",
        "            print('********************')\n",
        "\n",
        "\n",
        "    return optimizer, accuracies, precisions, recalls, fScores, boolean_values, grads, epoch_counts"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVi9Z3B3Q80J"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "def get_cong_avoidance_results_10classes(epochs=100, min_cond=0.95, max_cond = 0.99, mult=1, lr=0.1, min_epochs = 5):\n",
        "\n",
        "    '''Allow the congestion condition to change linearly over time '''\n",
        "\n",
        "    # Create ResNet model\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = ResNet18()\n",
        "    model = model.to(device)\n",
        "    if device == 'cuda':\n",
        "        print('CUDA device used...')\n",
        "        model = torch.nn.DataParallel(model)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # Import data\n",
        "    trainset, trainloader, testset, testloader = create_sampled_CIFAR10_data()\n",
        "    trainsets, trainloaders = create_class_subsets(trainset, shuffle=True, batch_size=128)\n",
        "    testsets, testloaders = create_class_subsets(testset, shuffle=False, batch_size=100)\n",
        "\n",
        "    # Create variables\n",
        "    cls_num = len(trainset.classes)\n",
        "    boolean_values = [False]*cls_num\n",
        "    grads = [{}]*cls_num\n",
        "    epoch_counts = [0]*cls_num\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # CREATE MODEL OPTIMIZER\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0, weight_decay=5e-4)\n",
        "    scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=lr, step_size_up=10, mode=\"triangular2\")\n",
        "\n",
        "    # BEGIN RECORDING THE TIME\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create matrices to store results\n",
        "\n",
        "    train_acc = np.zeros((epochs, cls_num))\n",
        "    train_P = np.zeros((epochs, cls_num))\n",
        "    train_R = np.zeros((epochs, cls_num))\n",
        "    train_F = np.zeros((epochs, cls_num))\n",
        "    test_acc = np.zeros((epochs, cls_num))\n",
        "    test_P = np.zeros((epochs, cls_num))\n",
        "    test_R = np.zeros((epochs, cls_num))\n",
        "    test_F = np.zeros((epochs, cls_num))\n",
        "    cong_events = np.zeros((epochs, cls_num))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print('\\n********** EPOCH {} **********'.format(epoch + 1))\n",
        "        print('Learning rate: ', optimizer.param_groups[0]['lr'])\n",
        "        confusion_matrix, accuracies, recalls, precisions, fScores, grads, epoch_counts = train_congestion_avoider_10classes(trainloaders, device, model, optimizer, criterion, boolean_values, grads, epoch_counts)\n",
        "        train_acc[epoch] = accuracies\n",
        "        train_P[epoch] = precisions\n",
        "        train_R[epoch] = recalls\n",
        "        train_F[epoch] = fScores\n",
        "        optimizer, accuracies, precisions, recalls, fScores, boolean_values, grads, epoch_counts = test_congestion_avoider_10classes(start_time, testloaders, device, model, optimizer, scheduler, grads, criterion, epoch, epochs, min_cond, max_cond, min_epochs, mult, epoch_counts)\n",
        "        test_acc[epoch] = accuracies\n",
        "        test_P[epoch] = precisions\n",
        "        test_R[epoch] = recalls\n",
        "        test_F[epoch] = fScores\n",
        "        cong_events[epoch] = boolean_values\n",
        "\n",
        "    return train_acc, train_P, train_R, train_F, test_acc, test_P, test_R, test_F, cong_events"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhZYV54xX4qZ",
        "outputId": "62b8bcaf-d5b1-4386-afcd-2735f3fe9d79"
      },
      "source": [
        "get_cong_avoidance_results_10classes(epochs=1, min_cond=0, max_cond = 0, mult=0, lr=0.1, min_epochs = 5)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA device used...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "********** EPOCH 1 **********\n",
            "Learning rate:  0.0001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time: 7.951 sec\n",
            "[[  2.   6. 372. 475. 137.   8.   0.   0.   0.   0.]\n",
            " [  1.   2. 245. 268.  74.   7.   1.   1.   0.   0.]\n",
            " [  0.   2. 163. 148.  42.   4.   0.   0.   0.   0.]\n",
            " [  0.   2.  98.  89.  25.   1.   0.   0.   0.   0.]\n",
            " [  1.   2.  65.  43.  18.   0.   0.   0.   0.   0.]\n",
            " [  0.   1.  24.  37.  14.   1.   0.   0.   0.   0.]\n",
            " [  0.   0.  18.  21.   5.   2.   0.   0.   0.   0.]\n",
            " [  0.   0.  12.  10.   4.   1.   0.   0.   0.   0.]\n",
            " [  1.   0.   5.   9.   1.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   5.   5.   0.   0.   0.   0.   0.   0.]]\n",
            "Class 0 A: : 0.081% (2/2478)\n",
            "Class 0 P: : 0.200% (2/1000)\n",
            "Class 0 R: : 40.000% (2/5)\n",
            "Class 0 F: : 0.398%\n",
            "********************\n",
            "Class 1 A: : 0.081% (2/2478)\n",
            "Class 1 P: : 0.334% (2/599)\n",
            "Class 1 R: : 13.333% (2/15)\n",
            "Class 1 F: : 0.651%\n",
            "********************\n",
            "Class 2 A: : 6.578% (163/2478)\n",
            "Class 2 P: : 45.404% (163/359)\n",
            "Class 2 R: : 16.187% (163/1007)\n",
            "Class 2 F: : 23.865%\n",
            "********************\n",
            "Class 3 A: : 3.592% (89/2478)\n",
            "Class 3 P: : 41.395% (89/215)\n",
            "Class 3 R: : 8.054% (89/1105)\n",
            "Class 3 F: : 13.485%\n",
            "********************\n",
            "Class 4 A: : 0.726% (18/2478)\n",
            "Class 4 P: : 13.953% (18/129)\n",
            "Class 4 R: : 5.625% (18/320)\n",
            "Class 4 F: : 8.018%\n",
            "********************\n",
            "Class 5 A: : 0.040% (1/2478)\n",
            "Class 5 P: : 1.299% (1/77)\n",
            "Class 5 R: : 4.167% (1/24)\n",
            "Class 5 F: : 1.980%\n",
            "********************\n",
            "Class 6 A: : 0.000% (0/2478)\n",
            "Class 6 P: : 0.000% (0/46)\n",
            "Class 6 R: : 0.000% (0/1)\n",
            "Class 6 F: : nan%\n",
            "********************\n",
            "Class 7 A: : 0.000% (0/2478)\n",
            "Class 7 P: : 0.000% (0/27)\n",
            "Class 7 R: : 0.000% (0/1)\n",
            "Class 7 F: : nan%\n",
            "********************\n",
            "Class 8 A: : 0.000% (0/2478)\n",
            "Class 8 P: : 0.000% (0/16)\n",
            "Class 8 R: : nan% (0/0)\n",
            "Class 8 F: : nan%\n",
            "********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.31984524, 0.06714493, 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ]]),\n",
              " array([[0.7936    , 0.27794461, 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ]]),\n",
              " array([[0.64699168, 0.23786408, 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ]]),\n",
              " array([[0.71283571, 0.25634713,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan, 0.        ]]),\n",
              " array([[0.0008071 , 0.0008071 , 0.06577885, 0.03591606, 0.00726392,\n",
              "         0.00040355, 0.        , 0.        , 0.        , 0.        ]]),\n",
              " array([[0.002     , 0.0033389 , 0.454039  , 0.41395349, 0.13953488,\n",
              "         0.01298701, 0.        , 0.        , 0.        , 0.        ]]),\n",
              " array([[0.4       , 0.13333333, 0.16186693, 0.08054299, 0.05625   ,\n",
              "         0.04166667, 0.        , 0.        ,        nan, 0.        ]]),\n",
              " array([[0.0039801 , 0.00651466, 0.238653  , 0.13484848, 0.08017817,\n",
              "         0.01980198,        nan,        nan,        nan, 0.        ]]),\n",
              " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}