{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "class IMBALANCECIFAR10(torchvision.datasets.CIFAR10):\n",
    "    cls_num = 10\n",
    "\n",
    "    def __init__(self, root, imb_type='exp', imb_factor=0.01, rand_number=0, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(IMBALANCECIFAR10, self).__init__(root, train, transform, target_transform, download)\n",
    "        np.random.seed(rand_number)\n",
    "        img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imb_factor)\n",
    "        self.gen_imbalanced_data(img_num_list)\n",
    "\n",
    "    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n",
    "        img_max = len(self.data) / cls_num\n",
    "        img_num_per_cls = []\n",
    "        if imb_type == 'exp':\n",
    "            for cls_idx in range(cls_num):\n",
    "                num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
    "                img_num_per_cls.append(int(num))\n",
    "        elif imb_type == 'step':\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max))\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max * imb_factor))\n",
    "        else:\n",
    "            img_num_per_cls.extend([int(img_max)] * cls_num)\n",
    "        return img_num_per_cls\n",
    "\n",
    "    def gen_imbalanced_data(self, img_num_per_cls):\n",
    "        new_data = []\n",
    "        new_targets = []\n",
    "        targets_np = np.array(self.targets, dtype=np.int64)\n",
    "        classes = np.unique(targets_np)\n",
    "        # np.random.shuffle(classes)\n",
    "        self.num_per_cls_dict = dict()\n",
    "        for the_class, the_img_num in zip(classes, img_num_per_cls):\n",
    "            self.num_per_cls_dict[the_class] = the_img_num\n",
    "            idx = np.where(targets_np == the_class)[0]\n",
    "            np.random.shuffle(idx)\n",
    "            selec_idx = idx[:the_img_num]\n",
    "            new_data.append(self.data[selec_idx, ...])\n",
    "            new_targets.extend([the_class, ] * the_img_num)\n",
    "        new_data = np.vstack(new_data)\n",
    "        self.data = new_data\n",
    "        self.targets = new_targets\n",
    "        \n",
    "    def get_cls_num_list(self):\n",
    "        cls_num_list = []\n",
    "        for i in range(self.cls_num):\n",
    "            cls_num_list.append(self.num_per_cls_dict[i])\n",
    "        return cls_num_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def create_sampled_CIFAR10_data():\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "    transform_val = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "    trainset = IMBALANCECIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = IMBALANCECIFAR10(root='./data', train=False, download=True, transform=transform_val, imb_factor=1)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)    \n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainset, trainloader, testset, testloader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "trainset, trainloader, testset, testloader = create_sampled_CIFAR10_data()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    '''\n",
    "    Implementation is taken from the PyTorch GitHub repository\n",
    "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "    '''\n",
    "\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    '''\n",
    "    Implementation is taken from the PyTorch GitHub repository\n",
    "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "    '''    \n",
    "    \n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    '''\n",
    "    Implementation is taken from the PyTorch GitHub repository\n",
    "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "    '''\n",
    "    \n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        \n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        \n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        \n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "class Bottleneck(nn.Module):\n",
    "    '''\n",
    "    Implementation is taken from the PyTorch GitHub repository\n",
    "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "    '''\n",
    "\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        \n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        \n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    '''\n",
    "    Implementation is taken from the PyTorch GitHub repository\n",
    "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], num_classes: int = 1000, zero_init_residual: bool = False, groups: int = 1, width_per_group: int = 64, replace_stride_with_dilation: Optional[List[bool]] = None, norm_layer: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        \n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        \n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        \n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        \n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        \n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        \n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        \n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        \n",
    "        self.inplanes = planes * block.expansion\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2], 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "\n",
    "img,targets = next(iter(trainloader))\n",
    "\n",
    "img= img.cuda()\n",
    "labels=torch.tensor(targets,dtype=torch.long,device='cuda')\n",
    "criterion= torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "outcome = model(img)\n",
    "\n",
    "print(outcome.shape)\n",
    "\n",
    "loss = criterion(outcome,labels)\n",
    "num_classes=10\n",
    "masks= [targets==k for k in range(num_classes)]\n",
    "sub_losses = [loss[m].sum() for m in masks]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/users/konsa15/.conda/envs/dds/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(sub_losses)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[tensor(93.8358, device='cuda:0', grad_fn=<SumBackward0>), tensor(85.9436, device='cuda:0', grad_fn=<SumBackward0>), tensor(48.4965, device='cuda:0', grad_fn=<SumBackward0>), tensor(15.8643, device='cuda:0', grad_fn=<SumBackward0>), tensor(41.5616, device='cuda:0', grad_fn=<SumBackward0>), tensor(22.6387, device='cuda:0', grad_fn=<SumBackward0>), tensor(7.6587, device='cuda:0', grad_fn=<SumBackward0>), tensor(2.0308, device='cuda:0', grad_fn=<SumBackward0>), tensor(5.8947, device='cuda:0', grad_fn=<SumBackward0>), tensor(2.5182, device='cuda:0', grad_fn=<SumBackward0>)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# a=torch.zeros_like(10)\n",
    "for layer,param in enumerate(model.parameters()):\n",
    "        param.grad = None\n",
    "grad_dict=[]\n",
    "for k,sl in enumerate(sub_losses):\n",
    "    sl.backward(retain_graph=True)\n",
    "    grad_dict.append({})\n",
    "    for layer,param in enumerate(model.parameters()):\n",
    "        grad_dict[k][f'layer_{layer}']=param.grad\n",
    "        param.grad = None\n",
    "\n",
    "# loss.backward()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\n",
    "suma=0\n",
    "for i in range(num_classes):\n",
    "    suma+=torch.cat([l.view(-1) for k,l in grad_dict[i].items()]).sum()\n",
    "print(suma)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(-37318.3828, device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for layer,param in enumerate(model.parameters()):\n",
    "        param.grad = None\n",
    "loss.sum().backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\n",
    "sumb=0\n",
    "for layer,param in enumerate(model.parameters()):\n",
    "        sumb+=param.grad.sum()\n",
    "print(sumb)\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(-37318.1992, device='cuda:0')\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('dds': conda)"
  },
  "interpreter": {
   "hash": "f40eeba64566014a0a3ffa06d5dcefa75af5e3f6b3455cff2b72928439d3f75d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}